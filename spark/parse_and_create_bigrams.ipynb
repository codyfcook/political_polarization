{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating bigram representation and feature set\n",
    "\n",
    "##### Parse into lists of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_in_tloc_line(line, is_senate): \n",
    "    if is_senate: \n",
    "        chamber = \"senate\"\n",
    "    else: \n",
    "        chamber = \"house\"\n",
    "    split_line = line.split(\",\")\n",
    "    if len(split_line)!=6: \n",
    "        return {\"date\":\"\", \"title\":\"\", \"speaker\":\"\", \"text\":\"\", \"party\":\"\", \"dwnom\":\"\"}\n",
    "    else: \n",
    "        party = split_line[4]\n",
    "        if party == u'200':\n",
    "            party = 1 # Republicans \n",
    "        else: \n",
    "            party = 0\n",
    "\n",
    "        return {\"date\":split_line[0], \"title\":split_line[1], \"speaker\":split_line[2], \"text\":split_line[3], \"party\":party, \"chamber\":chamber, \"dwnom1\":split_line[5]}\n",
    "\n",
    "def read_in_candidate_line(line): \n",
    "    split_line = line.split(\",\")\n",
    "    if len(split_line)!=4:\n",
    "        return {\"date\":\"\", \"speaker\":\"\", \"text\":\"\", \"type\":\"\"}\n",
    "    else: \n",
    "        return {\"date\":split_line[0], \"speaker\":split_line[1], \"text\":split_line[2], \"type\":split_line[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "house_raw = sc.textFile('../final_data/house_thomasloc_text.csv', minPartitions=20).map(lambda x: read_in_tloc_line(x, False))\n",
    "senate_raw = sc.textFile('../final_data/senate_thomasloc_text.csv', minPartitions=20).map(lambda x: read_in_tloc_line(x, True))\n",
    "all_cong = house_raw.union(senate_raw)\n",
    "candidates_raw = sc.textFile('../final_data/all_election_documents.csv', minPartitions=20).map(read_in_candidate_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_republicans = all_cong.filter(lambda x: x['party']==1)\n",
    "all_democrats = all_cong.filter(lambda x: x['party']==0) ## CHANGE THIS IF USING -1 FOR DEM \n",
    "all_cong = all_republicans.union(all_democrats)\n",
    "all_other_party = all_cong.filter(lambda x: x['party'] not in [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from stemming.porter2 import stem # Note: you need to install 'stemming' on each slave + master via ssh \n",
    "\n",
    "def parse_text(x): \n",
    "    ''' \n",
    "    Parses full text and returns words. Removes punctuation, numbers, and stop words. Stop word list is from http://xpo6.com/list-of-english-stop-words/\n",
    "    Also uses a porter stemmer to stem each word. So \"pour\" \"poured\" \"pouring\" would all become \"pour\" \n",
    "    '''\n",
    "    stopwords = open('stop_words.txt', 'r').read().split()\n",
    "    words = x['text'].lower()\n",
    "    words = re.sub('[^A-Za-z]+', ' ', words)\n",
    "    x['words'] = [w for w in words.split() if w not in stopwords]\n",
    "    x['words'] = [stem(w) for w in x['words']]\n",
    "    x['words'] = [w for w in x['words'] if len(w)>1]\n",
    "    x.pop(\"text\")\n",
    "    return x\n",
    "\n",
    "parsed_republicans = all_republicans.map(parse_text)\n",
    "parsed_democrats = all_democrats.map(parse_text)\n",
    "parsed_all_cong = all_cong.map(parse_text)\n",
    "parsed_candidates = candidates_raw.map(parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bigrams(x): \n",
    "    text = \" \".join(x['words'])\n",
    "    if len(text)<10: \n",
    "        x['bigrams'] = []\n",
    "        return x\n",
    "    bigrams = [b for b in zip(text.split(\" \")[:-1], text.split(\" \")[1:])]\n",
    "    x['bigrams'] = bigrams\n",
    "    x['num_bigrams'] = len(bigrams)\n",
    "    return x\n",
    "\n",
    "republican_bigrams = parsed_republicans.map(create_bigrams)\n",
    "democrat_bigrams = parsed_democrats.map(create_bigrams)\n",
    "candidates_bigrams = parsed_candidates.map(create_bigrams)\n",
    "all_cong_bigrams = parsed_all_cong.map(create_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "republican_bigrams_counts = republican_bigrams.flatMap(lambda x: x['bigrams']).countByValue().items()\n",
    "republican_bigrams_counts.sort(key = lambda x: x[1], reverse=True)\n",
    "\n",
    "democrat_bigrams_counts = democrat_bigrams.flatMap(lambda x: x['bigrams']).countByValue().items()\n",
    "democrat_bigrams_counts.sort(key = lambda x: x[1], reverse=True)\n",
    "\n",
    "all_cong_bigrams_counts = all_cong_bigrams.flatMap(lambda x: x['bigrams']).countByValue().items()\n",
    "all_cong_bigrams_counts.sort(key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates_bigrams_counts = candidates_bigrams.flatMap(lambda x: x['bigrams']).countByValue().items()\n",
    "candidates_bigrams_counts.sort(key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(candidates_bigrams_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(all_cong_bigrams_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of num of documents each bigram appears in \n",
    "def drop_duplicates(x): \n",
    "    temp = x['bigrams']\n",
    "    x['bigrams'] = list(temp)\n",
    "    return x\n",
    "\n",
    "bigrams_nodups = all_cong_bigrams.map(drop_duplicates)\n",
    "bigrams_doc_count = bigrams_nodups.flatMap(lambda x: x['bigrams']).countByValue().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sorted_bigrams_doc_count = sorted(bigrams_doc_count, key=lambda x: x[1], reverse=True)\n",
    "doc_count = np.array([x[1] for x in sorted_bigrams_doc_count])\n",
    "print \"The 99th ptile is %d documents\" % np.percentile(doc_count, 99)\n",
    "print \"The 90th ptile is %d documents\" % np.percentile(doc_count, 90)\n",
    "print \"The 80th ptile is %d documents\" % np.percentile(doc_count, 80)\n",
    "print \"The 75th ptile is %d documents\" % np.percentile(doc_count, 75)\n",
    "print \"The 60th ptile is %d documents\" % np.percentile(doc_count, 60)\n",
    "print \"The 50th ptile is %d documents\" % np.percentile(doc_count, 50)\n",
    "print \"The 40th ptile is %d documents\" % np.percentile(doc_count, 40)\n",
    "print \"The 25th ptile is %d documents\" % np.percentile(doc_count, 25)\n",
    "print \"The 15th ptile is %d documents\" % np.percentile(doc_count, 15)\n",
    "print \"The 5th ptile is %d documents\" % np.percentile(doc_count, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finalizing feature set \n",
    "Based on the congressional bigrams (since that will be the training set). \n",
    "\n",
    "Parameters: \n",
    "- Needs to appear in >15 documents\n",
    "- Needs to be mentioned 20 times\n",
    "- Needs to be spoken by 5 unique individuals\n",
    "- (inactive -- unclear if valuabe) If in both top 50 Republican and top 50 Democrat, not worth keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "def create_bigrams_rep_and_map(all_bigrams, dem_bigrams, rep_bigrams, doc_counts):\n",
    "    shuffle(all_bigrams)\n",
    "    \n",
    "    # PARAMETERS\n",
    "    doc_cutoff = 15\n",
    "    top_both_cutoff = 50\n",
    "    total_mention_cutoff = 20\n",
    "    \n",
    "    # keep only with over cutoff mentions\n",
    "    rv = [l[0] for l in all_bigrams if l[1]>total_mention_cutoff]\n",
    "    \n",
    "    # not in top total_mention_cutoff of both rep and dem bigrams\n",
    "#     top_rep = [l[0] for l in rep_bigrams[:top_both_cutoff]]\n",
    "#     top_dem = [l[0] for l in dem_bigrams[:top_both_cutoff]]\n",
    "#     top_both = [l for l in top_rep if l in top_dem]\n",
    "#     rv = [l for l in rv if l not in top_both]\n",
    "    \n",
    "    # in more than doc_cutoff documents \n",
    "    bigrams_over_doc_cutoff = [l[0] for l in doc_counts if l[1]>doc_cutoff]\n",
    "    rv = [l for l in rv if l in bigrams_over_doc_cutoff]\n",
    "    \n",
    "    # Some of the most common procedural bigrams\n",
    "    procedural_bigrams = [(u'remind', u'speaker'), (u'rollcal', u'motion'), (u'hous', u'major'),(u'file', u'amend'),\n",
    "                          (u'rollcal', u'motion'), (u'introduc', u'resolut'), (u'book', u'call'), (u'speaker', u'rise'),\n",
    "                          (u'amend', u'minut'), (u'presid', u'speaker'), (u'oppos', u'bill'), (u'speaker', u'democrat'), (u'hous', u'chamber'), \n",
    "                          (u'congression', u'poverti'), (u'seventh', u'congression'), (u'amend', u'minut'), (u'opposit', u'call'), \n",
    "                          (u'inquir', u'schedul'),  (u'schedul', u'week'),  (u'bill', u'text'),  (u'move', u'suspend'),  (u'purpos', u'clerk'), \n",
    "                          (u'mile', u'colorado'),  (u'address', u'floor'),  (u'page', u'regul'),  (u'hollen', u'chairman'),  (u'read', u'title'), \n",
    "                          (u'text', u'bill'),  (u'barbara', u'lee'), (u'time', u'ms'), (u'build', u'clerk'), (u'house', u'meet'),(u'meet', u'noon'),\n",
    "                          (u'congresswoman', u'christensen'), (u'recomit', u'desk'), (u'capitol', u'due'), (u'read', u'titl'),(u'purpos', u'inquir'),\n",
    "                          (u'progress', u'messag'), (u'leader', u'harri'),(u'yield', u'floor'), (u'due', u'prior'),(u'madam', u'presid'), (u'recommit', u'desk'), \n",
    "                          (u'madam', u'speaker'), (u'piec', u'legisl'),(u'speaker', u'thank'),(u'thank', u'chairman'), (u'yield', u'time'), (u'speaker', u'yield'),\n",
    "                          (u'presid', u'yeild'), (u'amend', u'desk')]\n",
    "    rv = [l for l in rv if l not in procedural_bigrams]\n",
    "    # There are some words leftover that are procedural and we should just remove all bigrams that include them \n",
    "    rv = [l for l in rv if 'motion' not in l and 'speaker' not in l and 'dr' not in l and 'hour' not in l and 'rollcall' not in l]\n",
    "    rv = [l for l in rv if 'house' not in l and 'meet' not in l and 'noon' not in l and 'repres' not in l and 'speaker' not in l]\n",
    "    rv = [l for l in rv if 'monday' not in l and 'hous' not in l and 'announc' not in l and 'clerk' not in l and 'whip' not in l]\n",
    "    rv = [l for l in rv if 'floor' not in l and 'week' not in l and 'tuesday' not in l and 'pass' not in l and 'postpon' not in l]\n",
    "\n",
    "    # Finally, we need an mapping to help us locate the bigrams by their index more easily\n",
    "    bigram_to_index = {}\n",
    "    index_to_bigram = {}\n",
    "    i = 1\n",
    "    for bigram in rv: \n",
    "        bigram_to_index[bigram] = i\n",
    "        i+=1 \n",
    "        index_to_bigram[i] = bigram\n",
    "    \n",
    "    return rv, bigram_to_index, index_to_bigram\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams_list, bigram_to_index, index_to_bigram = create_bigrams_rep_and_map(all_cong_bigrams_counts, democrat_bigrams_counts, republican_bigrams_counts, sorted_bigrams_doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(bigrams_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to restrict to bigrams that have a certain number of unique speakers. If a bigram were to only be spoken by a single person (even multiple times), could cause identification problems. But first we need to filter or congressional speach down to only bigrams in the above representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_features(x):\n",
    "    x['features'] = [l for l in x['bigrams'] if l in bigrams_list]\n",
    "    x['num_bigrams'] = len(x['bigrams'])\n",
    "    x['num_features'] = len(x['features'])\n",
    "    x.pop('bigrams')\n",
    "    x.pop('words')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_features = all_cong_bigrams.map(filter_features).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_on_unique_speakers(bigrams_list, features, cutoff): \n",
    "    ''' \n",
    "    Drops bigrams with less than <cutoff> unique speakers\n",
    "    '''\n",
    "    counts = {}\n",
    "    for bg in bigrams_list: \n",
    "        counts[bg] = []\n",
    "    for speaker in features: \n",
    "        for bg in speaker['features']: \n",
    "            if speaker not in counts[bg]: \n",
    "                counts[bg].append(speaker)\n",
    "    rv = [] \n",
    "    for bg in counts: \n",
    "        if len(counts[bg])>=cutoff: \n",
    "            rv.append(bg)\n",
    "    return rv \n",
    "\n",
    "bigrams_temp = filter_on_unique_speakers(bigrams_list, temp_features, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(bigrams_temp)\n",
    "print len(bigrams_list)\n",
    "bigrams_temp = bigrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_bigrams = len(bigrams_list)\n",
    "bc_bigram_to_index = sc.broadcast(bigram_to_index)\n",
    "bc_index_to_bigram = sc.broadcast(index_to_bigram)\n",
    "print \"Our representation has %d bigrams\" %num_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply the filter features map to the congressional bigrams. Don't collect (hence why can't just use temp_features)\n",
    "all_cong_features = all_cong_bigrams.map(filter_features)\n",
    "all_cong_features_collected = temp_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collapsing to speaker level + summary statistics\n",
    "For the congressional bigrams, we don't really care about individual speeches. We want a dataset where each row is a speaker and the columns are all the features. So first, let's collapse it so we have (speaker)-(every feature spoken). We'll later turn this into count vectors. \n",
    "\n",
    "Also, good time to check some basic summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_features = {}\n",
    "for f in all_cong_features_collected: \n",
    "    name = f['speaker']\n",
    "    if name not in final_features: \n",
    "        final_features[name] = {} \n",
    "    if 'features' not in final_features[name]:\n",
    "        final_features[name]['features'] = []\n",
    "        final_features[name]['chamber'] = f['chamber']\n",
    "        final_features[name]['dwnom'] = f['dwnom1']\n",
    "        final_features[name]['party'] = f['party']\n",
    "    final_features[name]['features'] += f['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_speakers = set() \n",
    "ac = all_cong.collect()\n",
    "for speech in ac: \n",
    "    if (speech['speaker'], speech['party'], speech['chamber']) not in all_speakers: \n",
    "        all_speakers.add((speech['speaker'], speech['party'], speech['chamber']))\n",
    "all_speakers = [l for l in list(all_speakers)]\n",
    "print \"There are %d unique speakers\" % len(all_speakers)\n",
    "print \"%d of which are Republican\" % len([l for l in all_speakers if l[1]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_words(x): \n",
    "    x = len(x['text'].split())\n",
    "    return x\n",
    "num_speeches = all_cong.count()\n",
    "print \"Average words per speech: \" + str(all_cong.map(num_words).reduce(lambda a, b: a+b)/num_speeches)\n",
    "print \"Number of speeches: %d \" % num_speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_feats = [] \n",
    "for i in range(len(final_features.keys())):\n",
    "    num_feats.append(len(final_features[final_features.keys()[i]]['features']))\n",
    "print \"Average number of active features per speaker:\"\n",
    "reduce(lambda x, y: x+y, num_feats)/float(len(num_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_features(x): \n",
    "    x = len(x['features'])\n",
    "    return x\n",
    "print \"Average active features per speech: \" + str(all_cong_features.map(num_features).reduce(lambda a, b: a+b)/all_cong_features.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting congressional speech data for use in R\n",
    "To use these features in R, we need to have a consistent 2d representation where each row is a speaker and each column represents a feature -- this way we can store the count of # of times the speaker spoke a given feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix the bigrams index to make room for three new columns\n",
    "We need the 2d representation to have [speaker, party, dwnom] preceeding the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speaker_to_index = {} \n",
    "index_to_speaker = {}\n",
    "i = 0\n",
    "for speaker in all_speakers: \n",
    "    speaker_to_index[speaker[0]] = i\n",
    "    index_to_speaker[i] = speaker[0]\n",
    "    i+=1\n",
    "\n",
    "bigram_to_index_2 = {}\n",
    "i = 2\n",
    "for bg in bigrams_list: \n",
    "    bigram_to_index_2[bg] = i\n",
    "    i+=1\n",
    "index_to_bigram_2 = {}\n",
    "for key in bigram_to_index_2: \n",
    "    index_to_bigram_2[bigram_to_index_2[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_bg = len(bigrams_list)+2\n",
    "len_as = len(all_speakers)\n",
    "main = [[0]*len_bg for _ in range(len_as)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for speaker in final_features: \n",
    "    if final_features[speaker]['dwnom'] != 'dwnom':\n",
    "        main[speaker_to_index[speaker]][0] = int(final_features[speaker]['party'])\n",
    "        main[speaker_to_index[speaker]][1] = float(final_features[speaker]['dwnom'])\n",
    "        for feature in final_features[speaker]['features']: \n",
    "            main[speaker_to_index[speaker]][bigram_to_index_2[feature]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breaking into covars and feature vectors\n",
    "Adds column names, generally formats nicely, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "covars = np.asmatrix(main)[:,:2]\n",
    "cong2016 = np.asmatrix(main)[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "covars_speaker_names = [[l[0].replace(\".\", \"\")] for l in all_speakers]\n",
    "covars_colnames = [\"speaker\", \"party\", \"dwnom\"]\n",
    "covars = np.asmatrix(main)[:,:2].tolist()\n",
    "covars = np.asarray([covars_colnames] + [l[0]+l[1] for l in zip(covars_speaker_names, covars)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cong2016_list = cong2016.tolist()\n",
    "cong2016_colnames =  [l[0]+\".\"+l[1] for l in bigrams_list]\n",
    "cong2016_list = [cong2016_colnames] + cong2016_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "f = csv.writer(open(\"../final_data/cong2016_covars.csv\", \"wb+\"))\n",
    "for row in covars: \n",
    "    f.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2 = csv.writer(open(\"../final_data/cong2016_counts.csv\", \"wb+\"))\n",
    "for row in cong2016_list: \n",
    "    f2.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting candidate data for use in R\n",
    "Similarly, we need to have the candidate data saved in a way we can use in R. \n",
    "\n",
    "Instead of having each speaker-date be a row, we want to collapse to speaker-month-features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restrict to candidates of interest and combine all ways their name appears\n",
    "The different data sources have different ways of writing the candidates names. We want to restrict to a certain set of candidates and standardize the name. This dictionary provides a map to do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates_of_interest = {'Chris Christie':'christie','christie':'christie',\n",
    "                          'Hillary Clinton': 'clinton', 'clinton':'clinton',\n",
    "                          'Bernie Sanders': 'sanders', 'sanders':'sanders',\n",
    "                          'Ted Cruz':'cruz', 'cruz':'cruz', \n",
    "                          'Donald Trump':'trump', 'trump':'trump',\n",
    "                          'Rand Paul':'paul', 'paul':'paul',\n",
    "                          'Jeb Bush':'bush', 'bush':'bush',\n",
    "                          'Marco Rubio':'rubio', 'rubio':'rubio',\n",
    "                          'John Kasich':'kasich', 'kasich':'kasich', \n",
    "                          'Ben Carson':'carson', 'carson':'carson'\n",
    "                         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidate_features = candidates_bigrams.map(filter_features)\n",
    "cand_feats = candidate_features.collect()\n",
    "cand_feats = cand_feats[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_cand_feats = {}\n",
    "for item in cand_feats:\n",
    "    # Drop press releases. Often linked news articles\n",
    "    if item['type']=='press releases':\n",
    "        continue\n",
    "    name = item['speaker']\n",
    "    # Confirm we care about this speaker and substitute for the correct name\n",
    "    if name not in candidates_of_interest.keys(): \n",
    "        continue\n",
    "    name = candidates_of_interest[name]\n",
    "    date = item['date'].strip()[2:]\n",
    "    # Initialize new dict if one doesn't exist\n",
    "    if name not in final_cand_feats:\n",
    "        final_cand_feats[name] = {} \n",
    "    if date not in final_cand_feats[name]: \n",
    "        final_cand_feats[name][date] = {}\n",
    "    if 'features' not in final_cand_feats[name][date]:\n",
    "        final_cand_feats[name][date]['features'] = [] \n",
    "        final_cand_feats[name][date]['num_bigrams'] = 0\n",
    "        final_cand_feats[name][date]['num_features'] = 0\n",
    "    final_cand_feats[name][date]['features'] += [l[0]+\".\"+l[1] for l in item['features']]\n",
    "    final_cand_feats[name][date]['num_bigrams'] += item['num_bigrams']\n",
    "    final_cand_feats[name][date]['num_features'] += item['num_features']       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of all candidate-date combos\n",
    "from datetime import datetime\n",
    "all_cand_dates = set()\n",
    "for speaker in final_cand_feats:\n",
    "    for date in final_cand_feats[speaker]:\n",
    "        d = datetime.strptime(date[:8], \"%y-%m-%d\")\n",
    "        # turn to a monthly date\n",
    "        date_monthly = str(d.month) + \"-\" + str(d.year)\n",
    "        all_cand_dates.add(speaker+\"_\"+date_monthly)\n",
    "all_cand_dates = list(all_cand_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Intialize and fill main array \n",
    "len_cd = len(all_cand_dates)\n",
    "len_feats = len(cong2016_list[0])\n",
    "main_cand = [[0]*len_feats for _ in range(len_cd)]\n",
    "for speaker in final_cand_feats:\n",
    "    for date in final_cand_feats[speaker]:\n",
    "        d = datetime.strptime(date[:8], \"%y-%m-%d\")\n",
    "        date_monthly = str(d.month) + \"-\" + str(d.year)\n",
    "        key = speaker+\"_\"+date_monthly\n",
    "        ind_r = all_cand_dates.index(key)\n",
    "        for feat in final_cand_feats[speaker][date]['features']:\n",
    "            if feat in cong2016_list[0]: \n",
    "                ind_c = cong2016_list[0].index(feat)\n",
    "                main_cand[ind_r][ind_c]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add header of bigram names\n",
    "main_cand_final = [cong2016_list[0]] + main_cand\n",
    "assert(len(main_cand_final)==len_cd+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to csv \n",
    "f3 = csv.writer(open(\"../final_data/pres_cand_counts.csv\", \"wb+\"))\n",
    "for row in main_cand_final: \n",
    "    f3.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create csv of Candidate - Date that aligns with above \n",
    "f4 = csv.writer(open(\"../final_data/cand_date_labels.csv\", \"wb+\"))\n",
    "for cand in all_cand_dates: \n",
    "    rv = cand.split(\"_\")\n",
    "    f4.writerow([rv[0], rv[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of all candidates\n",
    "all_speakers = final_cand_feats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Intialize and fill main array \n",
    "len_cd = len(all_speakers)\n",
    "len_feats = len(cong2016_list[0])\n",
    "main_cand = [[0]*len_feats for _ in range(len_cd)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for speaker in final_cand_feats:\n",
    "    for date in final_cand_feats[speaker]:\n",
    "        ind_r = all_speakers.index(speaker)\n",
    "        for feat in final_cand_feats[speaker][date]['features']:\n",
    "            if feat in cong2016_list[0]: \n",
    "                ind_c = cong2016_list[0].index(feat)\n",
    "                main_cand[ind_r][ind_c]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_cand_final = [cong2016_list[0]] + main_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to csv \n",
    "f3 = csv.writer(open(\"../final_data/candidates_all_speech.csv\", \"wb+\"))\n",
    "for row in main_cand_final: \n",
    "    f3.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create csv of Candidate - Date that aligns with above \n",
    "f4 = csv.writer(open(\"../final_data/candidates_all_speech_labels.csv\", \"wb+\"))\n",
    "for cand in all_speakers: \n",
    "    f4.writerow([cand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
